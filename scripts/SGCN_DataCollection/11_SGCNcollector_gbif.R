# Name: 
# Purpose: 
# Author: Christopher Tracey
# Created: 2016-08-11
# Updated: 2016-08-17
#
# Updates:
# 2025-04-02 - major updates made by MMOORE to switch from occ_search() which is deprecated
# to occ_download which is more appropriate for these downloads
# 
#---------------------------------------------------------------------------------------------

# clear the environments
rm(list=ls())

# load packages
if (!requireNamespace("here", quietly = TRUE)) install.packages("here")
  require(here)
if (!requireNamespace("rgbif", quietly = TRUE)) install.packages("rgbif")
  require(rgbif)
if (!requireNamespace("purrr", quietly = TRUE)) install.packages("purrr")
require(purrr)
if (!requireNamespace("tidyr", quietly = TRUE)) install.packages("tidyr")
require(tidyr)
if (!requireNamespace("triebeard", quietly = TRUE)) install.packages("triebeard")
require(triebeard)

source(here::here("scripts","00_PathsAndSettings.r"))

# load the r data file
load(file=updateData)

# read in SGCN data
loadSGCN()

splist <- lu_sgcn$SNAME

# species already in Biotics
splist <- splist[which(!splist %in% SGCN_bioticsCPP)]

# gets the  keys for each species name, based on GBIF
keys <- sapply(splist, function(x) name_backbone(name=x)$speciesKey[1], USE.NAMES=FALSE)

### add some code to put out the list of species not found in GBIF
a1 <- which(sapply(keys,is.null))
missingGBIFsp <- splist[a1]
missingGBIFsp
rm(a1)
cat(length(missingGBIFsp),"of",length(splist), "species were not found in GBIF", "\n", "They are:", paste(missingGBIFsp, collapse=", "))

# gets rid of any null values generated by name_backbone in the case of unmatchable species names
keys1 <- keys[-(which(sapply(keys,is.null),arr.ind=TRUE))] #note: seems to break if there is only one item in the list... use for multiple species!
keys <- unlist(keys1, use.names=FALSE)

# input GBIF credentials for download.
user = "mollymoore22"
pwd = "thisismygbifpassword1987"
email = "mollymoore22@gmail.com"

# download data for species, pred_in determines keys that we're searching for,
# pred_within is the wkt string that defines PA boundary, query out records with spatial problems or no coordinates,
# query out negative records, and records before the cutoff year
(dat <- occ_download(
  pred_in("taxonKey", keys),
  pred_within('POLYGON((-80.577111647999971 42.018959019000079, -80.583025511999949 39.690462536000041, -77.681987232999973 39.68735201800007, -75.761816590999956 39.690666106000037, -75.678308913999956 39.790810226000076, -75.53064649099997 39.815101786000071, -75.411566911999955 39.776679135000052, -75.101245089999964 39.880029385000057, -75.09383042199994 39.944216030000064, -74.690932882999959 40.133570156000076, -74.690425973999936 40.17528313400004, -74.893196517999968 40.350896889000069, -74.914505704999954 40.415842984000051, -75.012247039999977 40.448477402000037, -75.004556583999943 40.522413349000033, -75.134560399999941 40.623471625000036, -75.136516799999981 40.723392383000032, -75.002409694999983 40.867515299000047, -75.082051382999964 40.971575944000051, -74.830463730999952 41.152763058000062, -74.768212647999974 41.271891205000031, -74.640518995999969 41.358839422000074, -74.709416559999966 41.454495330000043, -74.826329023999961 41.475865789000068, -74.936988959999951 41.521739840000066, -75.018029425999941 41.617276498000081, -75.012709979999954 41.733926517000043, -75.061642930999938 41.85481505100006, -75.218658916999971 41.904656042000056, -75.336705265999967 42.017618624000079, -77.511689405999959 42.017704281000078, -79.721693517999938 42.024739989000068, -79.715980736999938 42.353623043000027, -80.577111647999971 42.018959019000079))'), # simplified boundary of Pennsylvania.), 
  pred("hasGeospatialIssue", FALSE), 
  pred("hasCoordinate", TRUE), 
  pred("occurrenceStatus", "PRESENT"), 
  pred_gte("year", cutoffyear), 
  user = user, pwd = pwd, email = email,
  format = "SIMPLE_CSV"
))

# here we will wait until download has completed before moving on
occ_download_wait(dat)

# import downloaded data to dataframe that we can work with here
myData <- occ_download_get(dat) |>
  occ_download_import()

gbifdata <- myData # just changing the name so it backs up

# define data source and format column names to match ours
gbifdata$DataSource <- "GBIF"

names(gbifdata)[names(gbifdata)=='species'] <- 'SNAME'
names(gbifdata)[names(gbifdata)=='gbifID'] <- 'DataID'
names(gbifdata)[names(gbifdata)=='decimalLongitude'] <- 'Longitude'
names(gbifdata)[names(gbifdata)=='decimalLatitude'] <- 'Latitude'
names(gbifdata)[names(gbifdata)=='year'] <- 'LastObs'
names(gbifdata)[names(gbifdata)=='basisOfRecord'] <- 'Notes'

# pull out records with the least uncertainty
gbifdata <- gbifdata[which(gbifdata$coordinateUncertaintyInMeters<=200|is.na(gbifdata$coordinateUncertaintyInMeters)),]

#subset to the needed columns
gbifdata <- gbifdata[c("SNAME","DataID","DataSource","Notes","LastObs","Longitude","Latitude", "coordinateUncertaintyInMeters")]

#remove rows w/ missing data in coordinates - this should already be taken care of when downloading data, but just making sure.
gbifdata <- gbifdata %>% drop_na(Longitude, Latitude)

# use COA
gbifdata$useCOA <- ifelse(gbifdata$LastObs>=cutoffyear, "y", "n")
gbifdata$OccProb <- "k"


gbifdata <- merge(gbifdata, lu_sgcn, by=c('SNAME'), all.x=TRUE)

# delete any bird records because there are so many season issues with them
gbifdata <- gbifdata[which(gbifdata$TaxaGroup!="AB"),]

# create a spatial layer
gbif_sf <- st_as_sf(gbifdata, coords=c("Longitude","Latitude"), crs="+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0")
gbif_sf <- st_transform(gbif_sf, crs=customalbers) # reproject to the custom albers
gbif_sf <- gbif_sf[final_fields] # field alignment
arc.write(path=here::here("_data","output",updateName,"SGCN.gdb","srcpt_GBIF"), gbif_sf, overwrite=TRUE) # write a feature class into the geodatabase
gbif_buffer_sf <- st_buffer(gbif_sf, dist=100) # buffer by 100m
arc.write(path=here::here("_data","output",updateName,"SGCN.gdb","final_GBIF"), gbif_buffer_sf, overwrite=TRUE) # write a feature class into the geodatabase

